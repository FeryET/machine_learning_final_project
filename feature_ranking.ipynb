{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"data/processed/filled.np.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Gender Status                                           features\n",
      "0      28      F      H  [29.99383078996264, -45.86057111460573, -2.002...\n",
      "1      24      M      S  [19.732400698167893, 4.998593506385246, 4.1476...\n",
      "2      29      M      N  [-11.592710202177722, -15.346528626992365, -1....\n",
      "3      28      M      H  [23.354775505152173, -22.054154733699782, 5.18...\n",
      "4      25      M      N  [23.50105405966495, -41.22614300331204, -21.03...\n",
      "...   ...    ...    ...                                                ...\n",
      "2600   21      F      N  [-35.85415092051014, -22.576961021566664, -18....\n",
      "2601   24      F      H  [1.1883946041473443, -12.860573937604215, -8.1...\n",
      "2602   12      F      H  [14.337451745943092, 6.2856630123797, -16.1075...\n",
      "2603   25      M      H  [-23.829385914151345, 11.042148119541661, -10....\n",
      "2604    6      M      H  [10.750766504165492, 70.0228469515277, -27.622...\n",
      "\n",
      "[2545 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/processed/processed_data.json\") as jfile:\n",
    "    df = json.load(jfile)[\"df\"]\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/indices.json\") as jfile:\n",
    "    indices = json.load(jfile)\n",
    "\n",
    "\n",
    "# We created a set of indices for each subset, and it's important to keep \n",
    "# Test indices seperate from our hyperparameter tunings,\n",
    "# Only validation is tested for feature selection and etc\n",
    "\n",
    "train_indices = np.array(indices[\"train_indices\"])\n",
    "val_indices = np.array(indices[\"val_indices\"])\n",
    "test_indices = np.array(indices[\"test_indices\"])\n",
    "\n",
    "assert len(np.intersect1d(train_indices, test_indices)) == 0\n",
    "assert len(np.intersect1d(train_indices, val_indices)) == 0\n",
    "assert len(np.intersect1d(val_indices, test_indices)) == 0\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_val = X[val_indices]\n",
    "X_test = X[test_indices]\n",
    "\n",
    "\n",
    "feature_dims = np.array(indices[\"feature_dims\"])\n",
    "features = \"f1 f2 f3 f4 f5\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_encoder = LabelEncoder().fit(df[\"Gender\"])\n",
    "status_encoder = LabelEncoder().fit(df[\"Status\"])\n",
    "\n",
    "genders = gender_encoder.transform(df[\"Gender\"])\n",
    "status = status_encoder.transform(df[\"Status\"])\n",
    "\n",
    "gender_onehot = OneHotEncoder().fit_transform(genders.reshape(-1, 1))\n",
    "status_onehot = OneHotEncoder().fit_transform(status.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': (0, 512),\n",
       " 'f2': (512, 2048),\n",
       " 'f3': (2048, 4352),\n",
       " 'f4': (4352, 6912),\n",
       " 'f5': (6912, 7116)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boundaries in the X matrix, each feature set starts from a start_column and ends in a end_column\n",
    "feature_boundaries = {}\n",
    "start_idx = 0\n",
    "for f, feature_len in zip(features, feature_dims):\n",
    "    feature_boundaries[f] = (start_idx, start_idx + feature_len)\n",
    "    start_idx += feature_len\n",
    "feature_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Importance Using Random Forests\n",
    "\n",
    "\n",
    "We use a random forest clf to classify the most important features for each label, starting with age.\n",
    "\n",
    "First we calculate a \"leave feature out\" error rate, to compare, then we calculate direct feature importances on a PCA projected set of each original set, and show some metrics. \n",
    "\n",
    "First we define some custom helper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom pca for cross validation\n",
    "\n",
    "class CustomPCA(TransformerMixin):\n",
    "    def __init__(self, indices, explained_var=0.8):\n",
    "        super().__init__()\n",
    "        self.explained_var = explained_var\n",
    "        self.indices = indices\n",
    "        self.pcas = {}\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        for k in self.indices:\n",
    "            start_col, end_col = self.indices[k]\n",
    "            self.pcas[k] = Pipeline(\n",
    "                [\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"pca\", PCA(self.explained_var))\n",
    "                ]\n",
    "            )\n",
    "            self.pcas[k].fit_transform(X[:, start_col:end_col])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, **kwargs):\n",
    "        X_transform = []\n",
    "        for k in self.indices:\n",
    "            start_col, end_col = self.indices[k]\n",
    "            X_transform.append(self.pcas[k].transform(X[:, start_col:end_col]))\n",
    "        return np.concatenate(X_transform, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbaScorer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, clf, X, y):\n",
    "        '''\n",
    "            calls predict_proba function of clf and computes\n",
    "            a bayesian mse for the clf\n",
    "        '''\n",
    "        probs = clf.predict_proba(X)\n",
    "        n_class = len(np.unique(y))\n",
    "        one_hot = np.zeros((len(X), n_class))\n",
    "        one_hot[:, y] = 1\n",
    "        mse = ((probs - one_hot) ** 2).mean()\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportanceScorer:\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __call__(self, clf, X=None, y=None):\n",
    "        '''\n",
    "            reporting mean, max and std of importances of random forest\n",
    "        '''\n",
    "        result = []\n",
    "        for f in self.indices:\n",
    "            start_col, end_col = self.indices[f]\n",
    "            importances = clf.feature_importances_[start_col:end_col]\n",
    "            result.append({\n",
    "                \"feature set\": f,\n",
    "                \"max_importance\": importances.max(),\n",
    "                \"mean_importance\": importances.mean(),\n",
    "                \"std_importance\": importances.std(),\n",
    "            })\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting from high dimensional data to each feature set's PC space\n",
    "pca = CustomPCA(feature_boundaries).fit(X[train_indices])\n",
    "X_train_transform = pca.transform(X_train)\n",
    "X_val_transform = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': (0, 27), 'f2': (27, 63), 'f3': (63, 88), 'f4': (88, 95), 'f5': (95, 98)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous indices does not work, we need to find the new indices for each pc set\n",
    "transformed_feature_boundaries = {}\n",
    "start = 0\n",
    "for k in pca.pcas:\n",
    "    end = start + pca.pcas[k][\"pca\"].n_components_\n",
    "    transformed_feature_boundaries[k] = (start, end)\n",
    "    start = end\n",
    "transformed_feature_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### Gender ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  except_feature  accuracy  bayes_mse\n",
      "0             f1  0.897059   0.436301\n",
      "1             f2  0.950980   0.444856\n",
      "2             f3  0.955882   0.446529\n",
      "3             f4  0.936275   0.448694\n",
      "4             f5  0.941176   0.446873\n",
      "\n",
      "\n",
      "Error rate for training with only one feature set:\n",
      "\n",
      "  feature_set  accuracy  bayes_mse\n",
      "0          f1  0.931373   0.448379\n",
      "1          f2  0.906863   0.434452\n",
      "2          f3  0.901961   0.436767\n",
      "3          f4  0.647059   0.380532\n",
      "4          f5  0.710784   0.385537\n",
      "\n",
      "\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.213016         0.013251        0.039226\n",
      "1          f2        0.091364         0.009163        0.018296\n",
      "2          f3        0.090295         0.010236        0.017106\n",
      "3          f4        0.005980         0.003970        0.001142\n",
      "4          f5        0.017606         0.009554        0.005770\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_except_info = []\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, )\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    clf.fit(curr_X_train, genders[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = genders[val_indices]\n",
    "    gender_except_info.append(\n",
    "        {\n",
    "            \"except_feature\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "gender_only_info = []\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    start, end = transformed_feature_boundaries[f]\n",
    "    curr_X_train = X_train_transform[:, start:end]\n",
    "    curr_X_val = X_val_transform[:, start:end]\n",
    "    clf.fit(curr_X_train, genders[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = genders[val_indices]\n",
    "    gender_only_info.append(\n",
    "        {\n",
    "            \"feature_set\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train_transform, genders[train_indices])\n",
    "gender_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(clf)\n",
    "\n",
    "\n",
    "print(\"####\\n#### Gender ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(gender_except_info)}\\n\\n\")\n",
    "print(f\"Error rate for training with only one feature set:\\n\\n{pd.DataFrame(gender_only_info)}\\n\\n\")\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(gender_set_importances)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### Status ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  except_feature  accuracy  bayes_mse\n",
      "0             f1  0.382353   0.540415\n",
      "1             f2  0.387255   0.539167\n",
      "2             f3  0.387255   0.539987\n",
      "3             f4  0.352941   0.541879\n",
      "4             f5  0.352941   0.541067\n",
      "\n",
      "\n",
      "Error rate for training with only one feature set:\n",
      "\n",
      "  feature_set  accuracy  bayes_mse\n",
      "0          f1  0.343137   0.537074\n",
      "1          f2  0.426471   0.541638\n",
      "2          f3  0.348039   0.541013\n",
      "3          f4  0.377451   0.537888\n",
      "4          f5  0.372549   0.536202\n",
      "\n",
      "\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.012203         0.010276        0.000685\n",
      "1          f2        0.014114         0.010214        0.001358\n",
      "2          f3        0.012417         0.010094        0.001150\n",
      "3          f4        0.012022         0.010492        0.000799\n",
      "4          f5        0.010428         0.009683        0.000753\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_except_info = []\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, )\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    clf.fit(curr_X_train, status[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = status[val_indices]\n",
    "    status_except_info.append(\n",
    "        {\n",
    "            \"except_feature\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "status_only_info = []\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    start, end = transformed_feature_boundaries[f]\n",
    "    curr_X_train = X_train_transform[..., start:end]\n",
    "    curr_X_val = X_val_transform[..., start:end]\n",
    "    clf.fit(curr_X_train, status[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = status[val_indices]\n",
    "    status_only_info.append(\n",
    "        {\n",
    "            \"feature_set\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train_transform, status[train_indices])\n",
    "status_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(clf)\n",
    "\n",
    "\n",
    "print(\"####\\n#### Status ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(status_except_info)}\\n\\n\")\n",
    "print(f\"Error rate for training with only one feature set:\\n\\n{pd.DataFrame(status_only_info)}\\n\\n\")\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(status_set_importances)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### Age ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  feature        mae       mse\n",
      "0      f1  86.434132  6.079125\n",
      "1      f2  59.117000  4.532079\n",
      "2      f3  61.962560  4.780256\n",
      "3      f4  61.588169  4.746810\n",
      "4      f5  59.102492  4.635944\n",
      "\n",
      "\n",
      "Error rate for training with only one feature set:\n",
      "\n",
      "  feature         mae        mse\n",
      "0      f1   77.084181   5.003633\n",
      "1      f2   91.450984   6.100178\n",
      "2      f3   92.147626   6.269327\n",
      "3      f4  177.571151  10.586940\n",
      "4      f5  183.283707  10.598791\n",
      "\n",
      "\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.012203         0.010276        0.000685\n",
      "1          f2        0.014114         0.010214        0.001358\n",
      "2          f3        0.012417         0.010094        0.001150\n",
      "3          f4        0.012022         0.010492        0.000799\n",
      "4          f5        0.010428         0.009683        0.000753\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "age = df[\"Age\"].to_numpy()\n",
    "\n",
    "age_except_info = []\n",
    "\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    regressor.fit(curr_X_train, age[train_indices])\n",
    "    preds = regressor.predict(curr_X_val)\n",
    "    target = age[val_indices]\n",
    "    age_except_info.append(\n",
    "        {\n",
    "            \"feature\": f,\n",
    "            \"mae\": mean_squared_error(target, preds),\n",
    "            \"mse\": mean_absolute_error(target, preds),\n",
    "        }\n",
    "    )\n",
    "\n",
    "age_only_info = []\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    start, end = transformed_feature_boundaries[f]\n",
    "    curr_X_train = X_train_transform[..., start:end]\n",
    "    curr_X_val = X_val_transform[..., start:end]\n",
    "    regressor.fit(curr_X_train, age[train_indices])\n",
    "    preds = regressor.predict(curr_X_val)\n",
    "    target = age[val_indices]\n",
    "    age_only_info.append(\n",
    "        {\n",
    "            \"feature\": f,\n",
    "            \"mae\": mean_squared_error(target, preds),\n",
    "            \"mse\": mean_absolute_error(target, preds),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "regressor.fit(X_train_transform, age[train_indices])\n",
    "age_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(clf)\n",
    "\n",
    "\n",
    "print(\"####\\n#### Age ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(age_except_info)}\\n\\n\")\n",
    "print(f\"Error rate for training with only one feature set:\\n\\n{pd.DataFrame(age_only_info)}\\n\\n\")\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(age_set_importances)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "927038ae8190c10368ddd6b7c17e7998b1fce7f35984d4fe249a4ddf13c1f407"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}