{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVC, SVR, LinearSVC\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"data/processed/filled.np.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Age Gender Status                                           features\n",
      "0      28      F      H  [-22.657711933297467, 30.769295879945037, -23....\n",
      "1      24      M      S  [182.05834145671884, 20.138809568394194, -6.23...\n",
      "2      29      M      N  [-21.308495839973528, -12.517586239957701, -9....\n",
      "3      28      M      H  [-22.320687052787786, 22.945827273719978, -22....\n",
      "4      25      M      N  [-22.65158385249504, 25.13058078215149, -11.95...\n",
      "...   ...    ...    ...                                                ...\n",
      "2600   21      F      N  [-21.756444948496828, -33.34037201123824, -2.2...\n",
      "2601   24      F      H  [-20.832185096880092, 1.4025807238407926, -12....\n",
      "2602   12      F      H  [-20.441490886122775, 14.783729334551827, -7.3...\n",
      "2603   25      M      H  [-20.757076001239067, -24.569769281532093, -1....\n",
      "2604    6      M      H  [-20.658551689088036, 10.331795410271129, 40.6...\n",
      "\n",
      "[2545 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/processed/processed_data.json\") as jfile:\n",
    "    df = json.load(jfile)[\"df\"]\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/indices.json\") as jfile:\n",
    "    indices = json.load(jfile)\n",
    "\n",
    "\n",
    "# We created a set of indices for each subset, and it's important to keep \n",
    "# Test indices seperate from our hyperparameter tunings,\n",
    "# Only validation is tested for feature selection and etc\n",
    "\n",
    "train_indices = np.array(indices[\"train_indices\"])\n",
    "val_indices = np.array(indices[\"val_indices\"])\n",
    "test_indices = np.array(indices[\"test_indices\"])\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_val = X[val_indices]\n",
    "X_test = X[test_indices]\n",
    "\n",
    "\n",
    "feature_dims = np.array(indices[\"feature_dims\"])\n",
    "features = \"f1 f2 f3 f4 f5\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_encoder = LabelEncoder().fit(df[\"Gender\"])\n",
    "status_encoder = LabelEncoder().fit(df[\"Status\"])\n",
    "\n",
    "genders = gender_encoder.transform(df[\"Gender\"])\n",
    "status = status_encoder.transform(df[\"Status\"])\n",
    "\n",
    "gender_onehot = OneHotEncoder().fit_transform(genders.reshape(-1, 1))\n",
    "status_onehot = OneHotEncoder().fit_transform(status.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': (0, 512),\n",
       " 'f2': (512, 2048),\n",
       " 'f3': (2048, 4352),\n",
       " 'f4': (4352, 6912),\n",
       " 'f5': (6912, 7116)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boundaries in the X matrix, each feature set starts from a start_column and ends in a end_column\n",
    "feature_boundaries = {}\n",
    "start_idx = 0\n",
    "for f, feature_len in zip(features, feature_dims):\n",
    "    feature_boundaries[f] = (start_idx, start_idx + feature_len)\n",
    "    start_idx += feature_len\n",
    "feature_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set Importance Using Random Forests\n",
    "\n",
    "\n",
    "We use a random forest clf to classify the most important features for each label, starting with age.\n",
    "\n",
    "First we calculate a \"leave feature out\" error rate, to compare, then we calculate direct feature importances on a PCA projected set of each original set, and show some metrics. \n",
    "\n",
    "First we define some custom helper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom pca for cross validation\n",
    "\n",
    "class CustomPCA(TransformerMixin):\n",
    "    def __init__(self, indices, explained_var=0.8):\n",
    "        super().__init__()\n",
    "        self.explained_var = explained_var\n",
    "        self.indices = indices\n",
    "        self.pcas = {}\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        for k in self.indices:\n",
    "            start_col, end_col = self.indices[k]\n",
    "            self.pcas[k] = Pipeline(\n",
    "                [\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"pca\", PCA(self.explained_var))\n",
    "                ]\n",
    "            )\n",
    "            self.pcas[k].fit_transform(X[:, start_col:end_col])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, **kwargs):\n",
    "        X_transform = []\n",
    "        for k in self.indices:\n",
    "            start_col, end_col = self.indices[k]\n",
    "            X_transform.append(self.pcas[k].transform(X[:, start_col:end_col]))\n",
    "        return np.concatenate(X_transform, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbaScorer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, clf, X, y):\n",
    "        '''\n",
    "            calls predict_proba function of clf and computes\n",
    "            a bayesian mse for the clf\n",
    "        '''\n",
    "        probs = clf.predict_proba(X)\n",
    "        n_class = len(np.unique(y))\n",
    "        one_hot = np.zeros((len(X), n_class))\n",
    "        one_hot[:, y] = 1\n",
    "        mse = ((probs - one_hot) ** 2).mean()\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportanceScorer:\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "    \n",
    "    def __call__(self, clf, X=None, y=None):\n",
    "        '''\n",
    "            reporting mean, max and std of importances of random forest\n",
    "        '''\n",
    "        result = []\n",
    "        for f in self.indices:\n",
    "            start_col, end_col = self.indices[f]\n",
    "            importances = clf.feature_importances_[start_col:end_col]\n",
    "            result.append({\n",
    "                \"feature set\": f,\n",
    "                \"max_importance\": importances.max(),\n",
    "                \"mean_importance\": importances.mean(),\n",
    "                \"std_importance\": importances.std(),\n",
    "            })\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting from high dimensional data to each feature set's PC space\n",
    "pca = CustomPCA(feature_boundaries).fit(X[train_indices])\n",
    "X_train_transform = pca.transform(X_train)\n",
    "X_val_transform = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': (0, 27), 'f2': (27, 63), 'f3': (63, 88), 'f4': (88, 95), 'f5': (95, 98)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The previous indices does not work, we need to find the new indices for each pc set\n",
    "transformed_feature_boundaries = {}\n",
    "start = 0\n",
    "for k in pca.pcas:\n",
    "    end = start + pca.pcas[k][\"pca\"].n_components_\n",
    "    transformed_feature_boundaries[k] = (start, end)\n",
    "    start = end\n",
    "transformed_feature_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### Gender ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  except_feature  accuracy  bayes_mse\n",
      "0             f1  0.945098   0.433745\n",
      "1             f2  0.972549   0.446248\n",
      "2             f3  0.968627   0.446806\n",
      "3             f4  0.980392   0.446873\n",
      "4             f5  0.976471   0.446447\n",
      "\n",
      "\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.214381         0.013744        0.039419\n",
      "1          f2        0.097156         0.008957        0.017545\n",
      "2          f3        0.086250         0.009820        0.016765\n",
      "3          f4        0.005190         0.003885        0.000748\n",
      "4          f5        0.018484         0.011253        0.005418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gender_info = []\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, )\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    clf.fit(curr_X_train, genders[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = genders[val_indices]\n",
    "    gender_info.append(\n",
    "        {\n",
    "            \"except_feature\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"####\\n#### Gender ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(gender_info)}\\n\\n\")\n",
    "\n",
    "clf.fit(X_train_transform, genders[train_indices])\n",
    "gender_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(clf)\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(gender_set_importances)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### STATUS ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  except_feature  accuracy  bayes_mse\n",
      "0             f1  0.376471   0.541448\n",
      "1             f2  0.325490   0.541568\n",
      "2             f3  0.364706   0.541999\n",
      "3             f4  0.349020   0.542433\n",
      "4             f5  0.384314   0.542347\n",
      "\n",
      "\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.011925         0.010428        0.000597\n",
      "1          f2        0.011913         0.010126        0.000971\n",
      "2          f3        0.012903         0.010057        0.001272\n",
      "3          f4        0.011911         0.010374        0.001031\n",
      "4          f5        0.010364         0.009958        0.000402\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_info = []\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, )\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    clf.fit(curr_X_train, status[train_indices])\n",
    "    preds = clf.predict(curr_X_val)\n",
    "    target = status[val_indices]\n",
    "    status_info.append(\n",
    "        {\n",
    "            \"except_feature\": f,\n",
    "            \"accuracy\": accuracy_score(preds, target),\n",
    "            \"bayes_mse\": ProbaScorer()(clf, curr_X_train, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"####\\n#### STATUS ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(status_info)}\\n\\n\")\n",
    "\n",
    "clf.fit(X_train_transform, status[train_indices])\n",
    "status_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(clf)\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(status_set_importances)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "#### AGE ####\n",
      "####\n",
      "Error rate for leave one feature out:\n",
      "\n",
      "  except_feature        mse       mae\n",
      "0             f1  52.454019  4.500816\n",
      "1             f2  40.658732  3.682593\n",
      "2             f3  40.274289  3.661965\n",
      "3             f4  35.812158  3.510609\n",
      "4             f5  37.810984  3.643460\n",
      "\n",
      "\n",
      "fitted\n",
      "Collective Feature Importances:\n",
      "\n",
      "  feature set  max_importance  mean_importance  std_importance\n",
      "0          f1        0.240749         0.016119        0.044558\n",
      "1          f2        0.135947         0.011025        0.026809\n",
      "2          f3        0.043462         0.005574        0.008669\n",
      "3          f4        0.003905         0.002565        0.000651\n",
      "4          f5        0.005884         0.003536        0.001664\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "age = df[\"Age\"].to_numpy()\n",
    "\n",
    "age_info = []\n",
    "\n",
    "for f in features:\n",
    "    # all sets except one\n",
    "    indices = [i for k, (start, end) in transformed_feature_boundaries.items() for i in range(start, end) if k != f]\n",
    "    curr_X_train = X_train_transform[..., indices]\n",
    "    curr_X_val = X_val_transform[..., indices]\n",
    "    regressor.fit(curr_X_train, age[train_indices])\n",
    "    preds = regressor.predict(curr_X_val)\n",
    "    target = age[val_indices]\n",
    "    age_info.append(\n",
    "        {\n",
    "            \"except_feature\": f,\n",
    "            \"mse\": mean_squared_error(preds, target),\n",
    "            \"mae\": mean_absolute_error(preds, target),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"####\\n#### AGE ####\\n####\")\n",
    "\n",
    "print(f\"Error rate for leave one feature out:\\n\\n{pd.DataFrame(age_info)}\\n\\n\")\n",
    "\n",
    "regressor.fit(X_train_transform, age[train_indices])\n",
    "\n",
    "age_set_importances = FeatureImportanceScorer(transformed_feature_boundaries)(regressor)\n",
    "\n",
    "print(f\"Collective Feature Importances:\\n\\n{pd.DataFrame(age_set_importances)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "927038ae8190c10368ddd6b7c17e7998b1fce7f35984d4fe249a4ddf13c1f407"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}